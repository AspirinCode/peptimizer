{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import textdistance\n",
    "import random\n",
    "import math\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Dropout, Flatten, Activation, Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from nested_lstm import NestedLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_PATH = './model/pre_trained/generator-loss0.2505-acc0.8949-val_loss3.4282-val_acc0.5270.hdf5'\n",
    "GENERATOR_DATASET = './dataset/generator_dataset.txt'\n",
    "\n",
    "PREDICTOR_MODEL_PATH = './model/pre_trained/predictor-MSE_val_loss-0.1881.hdf5'\n",
    "PREDICTOR_STATS_JSON = './dataset/predictor_dataset_stats.json'\n",
    "PREDICTOR_DATASET = './dataset/predictor_dataset.csv'\n",
    "\n",
    "RADIUS = 3\n",
    "NBITS = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open(GENERATOR_DATASET).read()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_vocab = len(chars)\n",
    "n_chars = len(raw_text)\n",
    "\n",
    "seq_length = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(raw_text) - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    if seq_out != \"\\n\":\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(1024, input_shape=(5, 1), return_sequences=True))\n",
    "model_lstm.add(NestedLSTM(1024, depth=4, dropout=0.1, recurrent_dropout=0.0, return_sequences=True))\n",
    "model_lstm.add(LSTM(1024, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Activation('relu'))\n",
    "model_lstm.add(LSTM(512, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Activation('relu'))\n",
    "model_lstm.add(LSTM(512))\n",
    "model_lstm.add(Dropout(0.1))\n",
    "model_lstm.add(Dense(23, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.load_weights(GENERATOR_MODEL_PATH)\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed():\n",
    "    test_seed = dataX[np.random.randint(0, len(dataX)-1)]\n",
    "    while 0 in test_seed:\n",
    "        test_seed = dataX[np.random.randint(0, len(dataX)-1)]\n",
    "    return test_seed\n",
    "\n",
    "def generate_seed():    \n",
    "    pattern = random_seed()\n",
    "    seq = [int_to_char[value] for value in pattern]\n",
    "\n",
    "    for i in range(25):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model_lstm.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        if index != 0:\n",
    "            result = int_to_char[index]\n",
    "            seq += result\n",
    "            pattern.append(index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        else:\n",
    "            pattern = random_seed()\n",
    "\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PREDICTOR_STATS_JSON) as f:\n",
    "    dict_data = json.load(f)\n",
    "    \n",
    "model_cnn = load_model(PREDICTOR_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fingerprint_Generation:\n",
    "    def __init__(self, smiles,radius=RADIUS,nbits=NBITS):\n",
    "        self.lookupfps = {}\n",
    "        \n",
    "        for key, value in lookupsmiles.items():\n",
    "            mol = Chem.MolFromSmiles(value)\n",
    "            fp = np.array(Chem.GetMorganFingerprintAsBitVect(mol,radius,nbits))\n",
    "            self.lookupfps[key] = fp\n",
    "        self.lookupfps[' '] = np.zeros(self.lookupfps['A'].shape)\n",
    "    \n",
    "    def seq(self, seq):\n",
    "        fp = np.asarray([self.lookupfps[seq[i]] for i in range(len(seq))])\n",
    "        return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookupsmiles = {\n",
    "         '2': 'NC(CSC1=C(F)C(F)=C(C(F)=C1F)C1=C(F)C(F)=C(SCC(N)C(N)=O)C(F)=C1F)C(N)=O',\n",
    "         '3': 'CC(=O)CC1=CN(CCCCC(N)C(N)=O)N=N1',\n",
    "         'A': 'N[C@@H](C)C(O)=O',\n",
    "         'B': 'C(CN)C(=O)O',\n",
    "         'X': 'C(CCC(=O)O)CCN',\n",
    "         'R': 'N[C@@H](CCCNC(N)=N)C(O)=O', \n",
    "         'N': 'N[C@@H](CC(N)=O)C(O)=O', \n",
    "         'D': 'N[C@@H](CC(O)=O)C(O)=O', \n",
    "         'C': 'N[C@H](C(O)=O)CS', \n",
    "         'E': 'N[C@@H](CCC(O)=O)C(O)=O', \n",
    "         'Q': 'N[C@@H](CCC(N)=O)C(O)=O', \n",
    "         'G': 'NCC(O)=O', \n",
    "         'H': 'N[C@@H](CC1=CNC=N1)C(O)=O', \n",
    "         'I': 'N[C@@H]([C@@H](C)CC)C(O)=O', \n",
    "         'L': 'N[C@@H](CC(C)C)C(O)=O', \n",
    "         'K': 'N[C@@H](CCCCN)C(O)=O', \n",
    "         'M': 'N[C@@H](CCSC)C(O)=O', \n",
    "         'F': 'N[C@@H](CC1=CC=CC=C1)C(O)=O', \n",
    "         'P': 'O=C(O)[C@H]1NCCC1', \n",
    "         'S': 'N[C@@H](CO)C(O)=O', \n",
    "         'T': 'N[C@@H]([C@H](O)C)C(O)=O', \n",
    "         'W': 'N[C@@H](CC1=CNC2=C1C=CC=C2)C(O)=O', \n",
    "         'Y': 'N[C@@H](CC1=CC=C(O)C=C1)C(O)=O', \n",
    "         'V': 'N[C@@H](C(C)C)C(O)=O',\n",
    "         '@': 'N[C@@H](CSC1=C(C(F)=C(C(F)=C1F)C2=C(C(F)=C(C(F)=C2F)SC[C@@H](C(O)=O)N)F)F)C(O)=O',\n",
    "         '#': 'N[C@H](C(O)=O)CSC1=CC(SC[C@@H](N)C(O)=O)=CC(SC[C@H](N)C(O)=O)=C1'\n",
    "}\n",
    "\n",
    "fp = Fingerprint_Generation(lookupsmiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sequence, model):\n",
    "    features_max = 108\n",
    "    fp_seq = fp.seq(sequence)\n",
    "    n_rows = features_max - len(sequence)\n",
    "    shape_padding = (n_rows, NBITS)\n",
    "    padding_array = np.zeros(shape_padding)\n",
    "    fp_seq = np.concatenate((fp_seq, padding_array), axis = 0)\n",
    "    \n",
    "    return model.predict(np.asarray([fp_seq]))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('AA', model_cnn) # Note: Intensity is scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_AAS = [i for i in lookupsmiles.keys() if i not in '@#1234567890']\n",
    "MAX_LEN = 108\n",
    "\n",
    "SEQ_LIST = pd.read_csv(PREDICTOR_DATASET)['sequences'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_charge(sequence):\n",
    "    #http://www.chem.ucalgary.ca/courses/351/Carey5th/Ch27/ch27-1-4-2.html\n",
    "    acidic = [sequence.count('D'), sequence.count('E'), sequence.count('C'), sequence.count('Y')]\n",
    "    basic = [sequence.count('R'), sequence.count('K'), sequence.count('H')]\n",
    "\n",
    "    acidic_pKa = [math.pow(10, 3.65), math.pow(10, 4.25), math.pow(10, 8.37), math.pow(10, 10.46)]\n",
    "    basic_pKa = [math.pow(10, 10.76), math.pow(10, 9.74), math.pow(10, 7.59)]\n",
    "\n",
    "    basic_coeff = [x*(1/(x+math.pow(10, 7))) for x in basic_pKa]\n",
    "    acidic_coeff = [math.pow(10, 7)/(x+math.pow(10, 7)) for x in acidic_pKa]\n",
    "\n",
    "    charge = - sum(np.multiply(acidic_coeff, acidic)) + sum(np.multiply(basic_coeff, basic))\n",
    "    return charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletion(str_sequence):\n",
    "    str_sequence = str_sequence.split(\" \")[0]\n",
    "    toremove = random.randint(0, len(str_sequence) - 1)\n",
    "    new_str = str_sequence[:toremove] + str_sequence[toremove+1:] + \" \"\n",
    "    return new_str\n",
    "\n",
    "def insertion(str_sequence, aminoacids=ORIGINAL_AAS):\n",
    "    str_sequence = str_sequence.split(\" \")[0]\n",
    "    toinsert = random.randint(0, len(str_sequence))\n",
    "    new_str = str_sequence[:toinsert] + random.choice(aminoacids) + str_sequence[toinsert:]\n",
    "    return new_str[:MAX_LEN]\n",
    "\n",
    "def swap(str_sequence, aminoacids=ORIGINAL_AAS):\n",
    "    existingaas = [i for i in set(str_sequence) if i != \" \"]\n",
    "    aatoreplace = random.choice(existingaas)\n",
    "    aaindices = [index for index, value in enumerate(str_sequence) if value == aatoreplace]\n",
    "    indextoreplace = random.choice(aaindices)\n",
    "    new_str = str_sequence[:indextoreplace] + random.choice(aminoacids) + str_sequence[indextoreplace+1:]\n",
    "    return new_str\n",
    "\n",
    "def hybrid(str_sequence, hybrid_list = SEQ_LIST):\n",
    "    str_sequence = str_sequence.split(\" \")[0]\n",
    "    tohybrid = random.randint(0, len(str_sequence)-1)\n",
    "    hybridlen = random.randint(0, int((len(str_sequence) - tohybrid)/5))+1\n",
    "    hybrid_from = random.choice(hybrid_list)\n",
    "    leastlen = max(len(min(hybrid_list, key=len)), len(hybrid_from))\n",
    "    while leastlen < hybridlen:\n",
    "        hybrid_from = random.choice(hybrid_list)\n",
    "        leastlen = len(hybrid_from)\n",
    "        print (leastlen, hybridlen)\n",
    "    index_hybrid_max = leastlen-hybridlen\n",
    "    if index_hybrid_max > 0:\n",
    "        index_hybrid_max = index_hybrid_max - 1\n",
    "    index_hybrid = random.randint(0, index_hybrid_max)\n",
    "    hybrid_from = hybrid_from[index_hybrid:index_hybrid+hybridlen]\n",
    "    new_str = str_sequence[:tohybrid] + hybrid_from + str_sequence[tohybrid+hybridlen:]\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_list = [insertion, deletion, swap, hybrid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitnessfunc(sequence, i):\n",
    "    nn_pred = predict(sequence, model_cnn)\n",
    "    arg_count = (((sequence).count('R')) - dict_data['mean_R_count']) / dict_data['std_R_count']\n",
    "    len_count = (len(sequence) - dict_data['mean_len_seq'])/dict_data['std_len_seq']\n",
    "    charge = (net_charge(sequence) - dict_data['mean_charge'])/dict_data['std_charge']\n",
    "    \n",
    "    similarity_training = max([textdistance.jaro_winkler.similarity(sequence, reference)\n",
    "                               for reference in SEQ_LIST])\n",
    "    \n",
    "    max_similarity_predicted = 0\n",
    "    similarity_predicted = 0\n",
    "    \n",
    "    try:\n",
    "        for k in range(1, i+1):\n",
    "            predicted_sequences = list(Seq_df.at[k, 'new_dict'].keys())\n",
    "            similarity_predicted = max(np.asarray([\n",
    "                textdistance.jaro_winkler.similarity(sequence, predicted_sequences[j])\n",
    "                                              for j in range(len(predicted_sequences))]))\n",
    "            max_similarity_predicted = max(similarity_predicted, max_similarity_predicted)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    similarity = max(similarity_training, max_similarity_predicted)\n",
    "    \n",
    "    value =  0.5*nn_pred - 0.5*(\n",
    "        0.5*arg_count + 0.2*len_count - 0.1*charge + similarity)\n",
    "    \n",
    "    return value, nn_pred, arg_count, len_count, charge, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(sequence, i,\n",
    "                      max_attempts,\n",
    "                      fitnessfunc=fitnessfunc,\n",
    "                      mutation_list=mutation_list):\n",
    "    \n",
    "    T = 100 # Parameter for simulated annealing\n",
    "    \n",
    "    oldseq = sequence\n",
    "    for attempt in range(max_attempts):\n",
    "        mutation = random.choice(mutation_list)\n",
    "        oldvalue, nn_pred, arg_count, len_count, charge, similarity = fitnessfunc(oldseq, i)\n",
    "        newseq = mutation(oldseq)\n",
    "        newvalue, nn_pred, arg_count, len_count, charge, similarity = fitnessfunc(newseq, i)\n",
    "        \n",
    "        delta = newvalue - oldvalue\n",
    "        \n",
    "        if (newvalue * np.exp(-delta/T)) > oldvalue:\n",
    "            oldseq = newseq\n",
    "            Seq_df.at[i, 'new_dict'][newseq] = [newvalue, nn_pred, arg_count, len_count, charge, similarity]\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator -> (Predictor-Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_seeds = 2\n",
    "\n",
    "Seq_df = pd.DataFrame(columns=['seed', 'new_dict'])\n",
    "\n",
    "for counter in range(number_seeds):\n",
    "    Seq_df.at[counter, 'seed'] = generate_seed()\n",
    "    Seq_df.at[counter, 'new_dict'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor-Optimizer Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attempts = 10\n",
    "new_seq_dict = {}\n",
    "\n",
    "for i in range(Seq_df.shape[0]):\n",
    "    genetic_algorithm(Seq_df.at[i, 'seed'], i, max_attempts)\n",
    "    new_seq_dict.update(Seq_df.at[i, 'new_dict'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame.from_dict(new_seq_dict, orient='index')\n",
    "df_temp = df_temp.rename(columns = {\n",
    "    0:'value', \n",
    "    1:'norm_intensity', \n",
    "    2:'norm_arg_count', \n",
    "    3:'norm_len_count', \n",
    "    4:'charge',\n",
    "    5:'similarity'}\n",
    "                              )\n",
    "df_temp['arg_count'] = df_temp.index.str.count('R')\n",
    "df_temp['sequences'] = df_temp.index\n",
    "df_temp['net_charge'] = df_temp['sequences'].apply(net_charge)\n",
    "df_temp['len'] = df_temp.index.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df = pd.DataFrame({\n",
    "    'sequences':df_temp['sequences'].tolist(),\n",
    "    'intensity':(df_temp['norm_intensity']*dict_data['std_intensity'] + \n",
    "                                dict_data['mean_intensity']).tolist(),\n",
    "    'length':(df_temp['len']).tolist(),\n",
    "    'relative_Arg':(df_temp['arg_count']/df_temp['len']).tolist(),\n",
    "    'relative_charge':(df_temp['net_charge']/df_temp['len']).tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLMAT]",
   "language": "python",
   "name": "conda-env-MLMAT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
